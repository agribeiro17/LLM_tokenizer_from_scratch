{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "38185e11",
   "metadata": {},
   "source": [
    "# Reading in a short story as text sample into python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8655a9fe",
   "metadata": {},
   "source": [
    "## Step 1: Creating Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aad073fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of character: 20479\n",
      "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no \n"
     ]
    }
   ],
   "source": [
    "# Opening Dataset\n",
    "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "\n",
    "print(\"Total number of character:\", len(raw_text))\n",
    "print(raw_text[:99]) # prints the first 100 char of this file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2972c61",
   "metadata": {},
   "source": [
    "### When working with LLMS millions and thousands of books are used, but today we are using only one books just to practice and understand better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db153097",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello,', ' ', 'world.', ' ', 'This,', ' ', 'is', ' ', 'a', ' ', 'test.']\n"
     ]
    }
   ],
   "source": [
    "import re # Splits text to obtain token list, splits any given text based on the whitespaces within the text or any other char\n",
    "\n",
    "text = \"Hello, world. This, is a test.\"\n",
    "result = re.split(r'(\\s)', text) # the '\\s' splits text wherever white spaces are encountered, into individual tokens\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab164fd",
   "metadata": {},
   "source": [
    "### Now lets modify the regular expression splits on whitespaces '(\\s)' and change it to also split where the comma and period to be included as individual splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7a857ff0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', '', ' ', 'world', '.', '', ' ', 'This', ',', '', ' ', 'is', ' ', 'a', ' ', 'test', '.', '']\n"
     ]
    }
   ],
   "source": [
    "result = re.split(r'([,.]|\\s)', text)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3435786",
   "metadata": {},
   "source": [
    "### Now that the commas, periods are also included as its own token we can now remove also the whitespace "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dc13b46d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', 'world', '.', 'This', ',', 'is', 'a', 'test', '.']\n"
     ]
    }
   ],
   "source": [
    "# Removing whitespaces completely from the sentence\n",
    "result = [item for item in result if item.strip()] #item.strip() will return true if char in sentence or false for none (Whitespaces will not be returned due to being false)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e2e220",
   "metadata": {},
   "source": [
    "When developing a simple tokenizer, whether we should encode whitespaces as\n",
    "separate characters or just remove them depends on our application and its\n",
    "requirements. Removing whitespaces reduces the memory and computing\n",
    "requirements. However, keeping whitespaces can be useful if we train models that\n",
    "are sensitive to the exact structure of the text (for example, Python code, which is\n",
    "sensitive to indentation and spacing). Here, we remove whitespaces for simplicity\n",
    "and brevity of the tokenized outputs. Later, we will switch to a tokenization scheme\n",
    "that includes whitespaces."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b18d330",
   "metadata": {},
   "source": [
    "### Lets include more char for separate tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d32ec586",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', 'world', '.', 'Is', 'this', '--', 'a', 'test', '?']\n"
     ]
    }
   ],
   "source": [
    "text = \"Hello, world. Is this-- a test?\"\n",
    "\n",
    "# The following two lines of codes are our tokenization scheme\n",
    "result = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text) # Splits texts depending on char\n",
    "result = [item.strip() for item in result if item.strip()] # Remove whitespaces\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09180491",
   "metadata": {},
   "source": [
    "### Now lets apply these two statements of the tokenization scheme into the 'raw_text'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4804603f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'HAD', 'always', 'thought', 'Jack', 'Gisburn', 'rather', 'a', 'cheap', 'genius', '--', 'though', 'a', 'good', 'fellow', 'enough', '--', 'so', 'it', 'was', 'no', 'great', 'surprise', 'to', 'me', 'to', 'hear', 'that', ',', 'in']\n",
      "Token Length: 4690\n"
     ]
    }
   ],
   "source": [
    "preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', raw_text)\n",
    "preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "\n",
    "print(preprocessed[:30]) # prints first 30 char. from file\n",
    "print(\"Token Length:\", len(preprocessed))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46539428",
   "metadata": {},
   "source": [
    "Currently our tokens are not in numerical representation rather still in words/char representations. \n",
    "Next step will be doing **Vocabulary**.\n",
    "\n",
    "The Vocabulary contains unique tokens and token IDs. Should be mapped in alphabetical order. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f991c90d",
   "metadata": {},
   "source": [
    "## Step 2: Creating Token IDs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2826072",
   "metadata": {},
   "source": [
    "In the previous section, we tokenized Edith Wharton's short story and assigned it to a\n",
    "Python variable called preprocessed. Let's now create a list of all unique tokens and sort\n",
    "them alphabetically to determine the vocabulary size:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4e0a0406",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of Vocab: 1130\n"
     ]
    }
   ],
   "source": [
    "# Sorting the set in alphabetical order\n",
    "all_words = sorted(set(preprocessed))\n",
    "\n",
    "# Print Vocab size\n",
    "vocab_len = len(all_words)\n",
    "print(\"Size of Vocab:\", vocab_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a4b1fa",
   "metadata": {},
   "source": [
    "We can see that the vocab size is 1130, indicating that it is less than the size of the *tokens*, which was expected as the vocab size only includes *unique tokens/words*!\n",
    "\n",
    "Remember that a vocab consists of a dictionary of tokens and its associate token IDs\n",
    "\n",
    "After determining that the vocabulary size is 1,130 via the above code, we create the vocabulary and print its first 51 entries for illustration purposes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c4a0767b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('!', 0)\n",
      "('\"', 1)\n",
      "(\"'\", 2)\n",
      "('(', 3)\n",
      "(')', 4)\n",
      "(',', 5)\n",
      "('--', 6)\n",
      "('.', 7)\n",
      "(':', 8)\n",
      "(';', 9)\n",
      "('?', 10)\n",
      "('A', 11)\n",
      "('Ah', 12)\n",
      "('Among', 13)\n",
      "('And', 14)\n",
      "('Are', 15)\n",
      "('Arrt', 16)\n",
      "('As', 17)\n",
      "('At', 18)\n",
      "('Be', 19)\n",
      "('Begin', 20)\n",
      "('Burlington', 21)\n",
      "('But', 22)\n",
      "('By', 23)\n",
      "('Carlo', 24)\n",
      "('Chicago', 25)\n",
      "('Claude', 26)\n",
      "('Come', 27)\n",
      "('Croft', 28)\n",
      "('Destroyed', 29)\n",
      "('Devonshire', 30)\n",
      "('Don', 31)\n",
      "('Dubarry', 32)\n",
      "('Emperors', 33)\n",
      "('Florence', 34)\n",
      "('For', 35)\n",
      "('Gallery', 36)\n",
      "('Gideon', 37)\n",
      "('Gisburn', 38)\n",
      "('Gisburns', 39)\n",
      "('Grafton', 40)\n",
      "('Greek', 41)\n",
      "('Grindle', 42)\n",
      "('Grindles', 43)\n",
      "('HAD', 44)\n",
      "('Had', 45)\n",
      "('Hang', 46)\n",
      "('Has', 47)\n",
      "('He', 48)\n",
      "('Her', 49)\n",
      "('Hermia', 50)\n"
     ]
    }
   ],
   "source": [
    "# we will assign an integer values to all the words in vocab (create a token id)\n",
    "vocab = {token: integer for integer, token in enumerate(all_words)}\n",
    "\n",
    "# printing a visual representation of how the tokens are being assigned\n",
    "for i, item in enumerate(vocab.items()):\n",
    "    print(item)\n",
    "    if i >= 50:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5239321",
   "metadata": {},
   "source": [
    "As we can see, based on the output above, the dictionary contains individual tokens associated with unique integer labels. Later in this book, when we want to convert the outputs of an LLM from numbers back into text, we also need a way to turn token IDs into text. For this, we can create an inverse version of the vocabulary that maps token IDs back to corresponding text tokens.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c1a2646",
   "metadata": {},
   "source": [
    "- Let's implement a complete tokenizer class in Python.\n",
    "\n",
    "    - The class will have an encode method that splits text into tokens and carries out the string-to-integer mapping to produce token IDs via the vocabulary. \n",
    "\n",
    "    - In addition, we implement a decode method that carries out the reverse integer-to-string mapping to convert the token IDs back into text.\n",
    "- Step 1: Store the vocabulary as a class attribute for access in the encode and decode methods\n",
    "\n",
    "- Step 2: Create an inverse vocabulary that maps token IDs back to the original text tokens\n",
    "\n",
    "- Step 3: Process input text into token IDs\n",
    "\n",
    "- Step 4: Convert token IDs back into text\n",
    "\n",
    "- Step 5: Replace spaces before the specified punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1a8e1b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizerV1:\n",
    "    # this init method is called by default when called upon this class 'SimpleTokenizerV1', with the arguments which takes vocab\n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = {i:s for s, i in vocab.items()} # take the token and token_id in vocab and flip it, used for decoder method\n",
    "\n",
    "    # Exact same preprocessing text is introduced as done before for 'Encoder'\n",
    "    def encode(self, text):\n",
    "        preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "\n",
    "        # converting individual token to token ids\n",
    "        ids = [self.str_to_int[s] for s in preprocessed]\n",
    "        return ids\n",
    "\n",
    "    # we are using the reversed dictionary(int_to_str), and converting the token_id to individual tokens, then joining the individual tokens together\n",
    "    def decode(self, ids):\n",
    "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
    "\n",
    "        # Replace spaces before the specified punctuations\n",
    "        text = re.sub(r'\\s+([,.:;\"()\\'])', r'\\1', text) # getting rid of all the spaces before the punctuations to make a complete sentence\n",
    "        return text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "907f410f",
   "metadata": {},
   "source": [
    "### Let's instantiate a new tokenizer object from the SimpleTokenizerV1 class and tokenize a passage from Edith Wharton's short story to try it out in practice:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e94f4995",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 56, 2, 850, 988, 602, 533, 746, 5, 1126, 596, 5, 1, 67, 7, 38, 851, 1108, 754, 793, 7]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = SimpleTokenizerV1(vocab)\n",
    "text = \"\"\"\"It's the last he painted, you know,\"\n",
    "           Mrs. Gisburn said with pardonable pride.\"\"\"\n",
    "\n",
    "ids = tokenizer.encode(text)\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3744304",
   "metadata": {},
   "source": [
    "### Next, let's see if we can turn these token IDs back into text using the decode method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "85614ae1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\" It\\' s the last he painted, you know,\" Mrs. Gisburn said with pardonable pride.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7bcc4e7",
   "metadata": {},
   "source": [
    "The text that was implemented in 'text' was used from the training set the \"raw_text\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a8e835",
   "metadata": {},
   "source": [
    "### Let's now apply it to a new text sample that is not contained in the training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2984f502",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Hello'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHello do you like Tea?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28mprint\u001b[39m(tokenizer\u001b[38;5;241m.\u001b[39mencode(text))\n",
      "Cell \u001b[0;32mIn[10], line 13\u001b[0m, in \u001b[0;36mSimpleTokenizerV1.encode\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m     10\u001b[0m preprocessed \u001b[38;5;241m=\u001b[39m [item\u001b[38;5;241m.\u001b[39mstrip() \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m preprocessed \u001b[38;5;28;01mif\u001b[39;00m item\u001b[38;5;241m.\u001b[39mstrip()]\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# converting individual token to token ids\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m ids \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstr_to_int[s] \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m preprocessed]\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ids\n",
      "Cell \u001b[0;32mIn[10], line 13\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     10\u001b[0m preprocessed \u001b[38;5;241m=\u001b[39m [item\u001b[38;5;241m.\u001b[39mstrip() \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m preprocessed \u001b[38;5;28;01mif\u001b[39;00m item\u001b[38;5;241m.\u001b[39mstrip()]\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# converting individual token to token ids\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m ids \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstr_to_int[s] \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m preprocessed]\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ids\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Hello'"
     ]
    }
   ],
   "source": [
    "text = \"Hello do you like Tea?\"\n",
    "print(tokenizer.encode(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0adb7ddd",
   "metadata": {},
   "source": [
    "### An error is printed due to hello not being used in the 'vocab', which is the short story (raw_text). This highlights the need to consider large and diverse training sets to extend the vocabulary when working on LLMs. To deal with this issue we add special context tokens. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2bcc5f0",
   "metadata": {},
   "source": [
    "## ADDING SPECIAL CONTEXT TOKENS\n",
    "\n",
    "- In this section, we will modify this tokenizer to handle unknown\n",
    "words.\n",
    "\n",
    "- In particular, we will modify the vocabulary and tokenizer we implemented in the\n",
    "previous section, SimpleTokenizerV2, to support two new tokens, <|unk|> and\n",
    "<|endoftext|>\n",
    "\n",
    "- We can modify the tokenizer to use an <|unk|> token if it\n",
    "encounters a word that is not part of the vocabulary. \n",
    "\n",
    "- Furthermore, we add a token between\n",
    "unrelated texts. \n",
    "\n",
    "- For example, when training GPT-like LLMs on multiple independent\n",
    "documents or books, it is common to insert a token before each document or book that\n",
    "follows a previous text source\n",
    "\n",
    "- When working with multiple text sources, we add <|endoftext|> tokens between these texts. These tokens act as markers, signaling the start and end of a particular segment. This leads to more effective processing and understanding by the LLMs.\n",
    "  - if this token was not there it would join all text and mix everything up."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "684f60be",
   "metadata": {},
   "source": [
    "### Now lets modify the vocabulary to include these two special tokens, <|unk|> and <|endoftext|>, by adding these to the list of all unique words that we created in the previous section:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4d02e52b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('younger', 1127)\n",
      "('your', 1128)\n",
      "('yourself', 1129)\n",
      "('<|endoftext|>', 1130)\n",
      "('<|unk|>', 1131)\n"
     ]
    }
   ],
   "source": [
    "all_tokens = sorted(list(set(preprocessed)))\n",
    "\n",
    "# to add the new tokens use the .extend\n",
    "all_tokens.extend([\"<|endoftext|>\", \"<|unk|>\"])\n",
    "\n",
    "vocab = {token:integer for integer, token in enumerate(all_tokens)}\n",
    "\n",
    "len(vocab.items())\n",
    "for i, item in enumerate(list(vocab.items()) [-5:]):\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2504d77e",
   "metadata": {},
   "source": [
    "### Now lets update the SimpleTokenizer in our second version by replacing unknown words and spaces before the specified punctuations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4f0619e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizerV2:\n",
    "    # Remains the same\n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = {i:s for s,i in vocab.items()}\n",
    "\n",
    "    def encode(self, text):\n",
    "        preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "\n",
    "        # If not presented in vocab the token will be ID as unknown\n",
    "        preprocessed = [item if item in self.str_to_int else \"<|unk|>\" for item in preprocessed]\n",
    "\n",
    "        # converting individual token to token ids\n",
    "        ids = [self.str_to_int[s] for s in preprocessed]\n",
    "        return ids\n",
    "\n",
    "    # Stays the same\n",
    "    def decode(self, ids):\n",
    "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
    "        text = re.sub(r'\\s+([,.:;\"()\\'])', r'\\1', text) # getting rid of all the spaces before the punctuations to make a complete sentence\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9f58fd89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, do you like Tea?<|endoftext|>In the sunlit terraces of the palace.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = SimpleTokenizerV2(vocab)\n",
    "\n",
    "# Splitting text source like done in GPT\n",
    "text1 = \"Hello, do you like Tea?\"\n",
    "text2 = \"In the sunlit terraces of the palace.\"\n",
    "\n",
    "text = \"<|endoftext|>\".join((text1,text2))\n",
    "\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c5c6eb7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1131, 5, 355, 1126, 628, 1131, 10, 1131, 988, 956, 984, 722, 988, 1131, 7]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'<|unk|>, do you like <|unk|> ? <|unk|> the sunlit terraces of the <|unk|>.'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# No more errors due to dealing with unkown char\n",
    "print(tokenizer.encode(text))\n",
    "tokenizer.decode(tokenizer.encode(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0073e17",
   "metadata": {},
   "source": [
    "\n",
    "So far, we have discussed tokenization as an essential step in processing text as input to\n",
    "LLMs. Depending on the LLM, some researchers also consider additional special tokens such\n",
    "as the following:\n",
    "\n",
    "[BOS] (beginning of sequence): This token marks the start of a text. It\n",
    "signifies to the LLM where a piece of content begins.\n",
    "\n",
    "[EOS] (end of sequence): This token is positioned at the end of a text,\n",
    "and is especially useful when concatenating multiple unrelated texts,\n",
    "similar to <|endoftext|>. For instance, when combining two different\n",
    "Wikipedia articles or books, the [EOS] token indicates where one article\n",
    "ends and the next one begins.\n",
    "\n",
    "[PAD] (padding): When training LLMs with batch sizes larger than one,\n",
    "the batch might contain texts of varying lengths. To ensure all texts have\n",
    "the same length, the shorter texts are extended or \"padded\" using the\n",
    "[PAD] token, up to the length of the longest text in the batch.\n",
    "\n",
    "Note that the tokenizer used for GPT models does not need any of these tokens mentioned\n",
    "above but only uses an <|endoftext|> token for simplicity\n",
    "\n",
    "the tokenizer used for GPT models also doesn't use an <|unk|> token for outof-vocabulary words. Instead, GPT models use a byte pair encoding tokenizer, which breaks\n",
    "down words into subword units"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b16796",
   "metadata": {},
   "source": [
    "### BYTE PAIR ENCODING (BPE)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5fb1bfb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip3 install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b178deec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tiktoken version: 0.12.0\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import tiktoken\n",
    "\n",
    "print(\"tiktoken version:\", importlib.metadata.version(\"tiktoken\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c843eadf",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c408857d",
   "metadata": {},
   "source": [
    "The usage of this tokenizer is similar to SimpleTokenizerV2 we implemented previously via an encode method, but in one line in code from the tiktoken library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c2144abc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15496, 11, 466, 345, 588, 8887, 30, 220, 50256, 554, 262, 4252, 18250, 8812, 2114, 1659, 617, 2954, 593, 27271, 13]\n"
     ]
    }
   ],
   "source": [
    "text = (\"Hello, do you like tea? <|endoftext|> In the sunlit terraces\"\n",
    "        \"of someunkownPlace.\") # tokenizer is able to tokenize random words like this OOV and does not give error as before\n",
    "\n",
    "integers = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"}) # <|endoftext|> is part of GPT2 or GPT3\n",
    "print(integers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3598789",
   "metadata": {},
   "source": [
    "Now we can also decode the sentence back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1cdf0361",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, do you like tea? <|endoftext|> In the sunlit terracesof someunkownPlace.\n"
     ]
    }
   ],
   "source": [
    "strings = tokenizer.decode(integers)\n",
    "print(strings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32189308",
   "metadata": {},
   "source": [
    "\n",
    "We can make two noteworthy observations based on the token IDs and decoded text\n",
    "above. \n",
    "\n",
    "- **First**, the <|endoftext|> token is assigned a relatively large token ID, namely,50256. \n",
    "    - In fact, the BPE tokenizer, which was used to train models such as GPT-2, GPT-3, and the original model used in ChatGPT, has a total vocabulary size of 50,257, with <|endoftext|> being assigned the largest token ID.\n",
    "    \n",
    "- **Second**, the BPE tokenizer above encodes and decodes unknown words, such as \"someunknownPlace\" correctly. \n",
    "    - The BPE tokenizer can handle any unknown word. How does it achieve this without using <|unk|> tokens?\n",
    "- The algorithm underlying BPE breaks down words that aren't in its predefined vocabulary into smaller subword units or even individual characters.\n",
    "\n",
    " - The enables it to handle out-ofvocabulary words. \n",
    "\n",
    "- So, thanks to the BPE algorithm, if the tokenizer encounters an unfamiliar word during tokenization, it can represent it as a sequence of subword tokens or characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d98ff631",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10206, 21809, 220, 959]\n",
      "AKwire ier\n"
     ]
    }
   ],
   "source": [
    "integers = tokenizer.encode(\"AKwire ier\") # random words\n",
    "print(integers)\n",
    "\n",
    "string = tokenizer.decode(integers)\n",
    "print(string)\n",
    "\n",
    "# Still able to encode and decode very well"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d87f45",
   "metadata": {},
   "source": [
    "### Creating Input-Target Pairs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5b53a04",
   "metadata": {},
   "source": [
    "In this section we implement a data loader that fetches the input-target pairs using a sliding window approach\n",
    "\n",
    "To get started, we will first tokenize the whole The Verdict short story we worked with earlier using the BPE tokenizer introduced in the previous section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7719f9ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5145\n"
     ]
    }
   ],
   "source": [
    "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "\n",
    "enc_text = tokenizer.encode(raw_text)\n",
    "print(len(enc_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf35c724",
   "metadata": {},
   "source": [
    "Next, we remove the first 50 tokens from the dataset for demonstration purposes as it\n",
    "results in a slightly more interesting text passage in the next steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a31483d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_sample = enc_text[50:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc936c33",
   "metadata": {},
   "source": [
    "One of the easiest and most intuitive ways to create the input-target pairs for the nextword prediction task is to create two variables, x and y, where x contains the input tokens\n",
    "and y contains the targets, which are the inputs shifted by 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "28a0d9fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: [290, 4920, 2241, 287]\n",
      "y: [4920, 2241, 287, 257]\n"
     ]
    }
   ],
   "source": [
    "context_size = 4 #length of the input\n",
    "#The context_size of 4 means that the model is trained to look at a sequence of 4 words (or tokens)\n",
    "#to predict the next word in the sequence.\n",
    "#The input x is the first 4 tokens [1, 2, 3, 4], and the target y is the next 4 tokens [2, 3, 4, 5]\n",
    "\n",
    "x = enc_sample[:context_size]\n",
    "y = enc_sample[1:context_size + 1]\n",
    "\n",
    "print(f\"x: {x}\")\n",
    "print(f\"y: {y}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85cb3402",
   "metadata": {},
   "source": [
    "Processing the inputs along with the targets, which are the inputs shifted by one position, we can then create the next-word prediction tasks as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9ac760f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[290] -----> 4920\n",
      "[290, 4920] -----> 2241\n",
      "[290, 4920, 2241] -----> 287\n",
      "[290, 4920, 2241, 287] -----> 257\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, context_size + 1):\n",
    "    context = enc_sample[:i]\n",
    "    desired = enc_sample[i]\n",
    "\n",
    "    print(context, \"----->\", desired)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d02080a",
   "metadata": {},
   "source": [
    "Everything left of the arrow (---->) refers to the input an LLM would receive, and the token\n",
    "ID on the right side of the arrow represents the target token ID that the LLM is supposed to\n",
    "predict.\n",
    "\n",
    "For illustration purposes, let's repeat the previous code but convert the token IDs into\n",
    "text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "579e1c4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " and ---->  established\n",
      " and established ---->  himself\n",
      " and established himself ---->  in\n",
      " and established himself in ---->  a\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, context_size+1):\n",
    "    context = enc_sample[:i]\n",
    "    desired = enc_sample[i]\n",
    "\n",
    "    print(tokenizer.decode(context), \"---->\", tokenizer.decode([desired]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dedcefe8",
   "metadata": {},
   "source": [
    "There's only one more task before we can turn the tokens into embeddings:implementing an efficient data loader that\n",
    "iterates over the input dataset and returns the inputs and targets as PyTorch tensors, which\n",
    "can be thought of as multidimensional arrays.\n",
    "\n",
    "In particular, we are interested in returning two tensors: an input tensor containing the\n",
    "text that the LLM sees and a target tensor that includes the targets for the LLM to predict,\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a25db6cc",
   "metadata": {},
   "source": [
    "### Implementing Data Loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0ac6fe9",
   "metadata": {},
   "source": [
    "For the efficient data loader implementation, we will use PyTorch's built-in Dataset and\n",
    "DataLoader classes.\n",
    "\n",
    "    \n",
    "Step 1: Tokenize the entire text\n",
    "    \n",
    "Step 2: Use a sliding window to chunk the book into overlapping sequences of max_length\n",
    "\n",
    "Step 3: Return the total number of rows in the dataset\n",
    "\n",
    "Step 4: Return a single row from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "240b3772",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class GPTDatasetV1(Dataset):\n",
    "    def __init__(self, txt, tokenizer, max_length, stride):\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "\n",
    "        # Tokenize the entire text\n",
    "        token_ids = tokenizer.encode(txt, allowed_special={\"<|endoftext|>\"})\n",
    "\n",
    "        # Use a sliding window to chunk the book into overlapping sequences of max_length\n",
    "        # This part is defining the input output tensor\n",
    "        for i in range(0, len(token_ids) - max_length, stride):\n",
    "            input_chunk = token_ids[i:i + max_length]\n",
    "            target_chunk = token_ids[i+1:i+max_length+1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "\n",
    "    # Based on the index that we provide it just return the that particular row of the input\n",
    "    # as well as the particular row if the output/target tensor\n",
    "    # This is a map dataset so we must include this as well\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.target_ids[idx]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5899ceef",
   "metadata": {},
   "source": [
    "\n",
    "The GPTDatasetV1 class in listing 2.5 is based on the PyTorch Dataset class.\n",
    "\n",
    "It defines how individual rows are fetched from the dataset. \n",
    "\n",
    "Each row consists of a number of\n",
    "token IDs (based on a max_length) assigned to an input_chunk tensor. \n",
    "\n",
    "The target_chunk\n",
    "tensor contains the corresponding targets. \n",
    "\n",
    "Now the following code will use the GPTDatasetV1 to load the inputs in batches via a PyTorch\n",
    "DataLoader\n",
    "\n",
    "Step 1: Initialize the tokenizer\n",
    "\n",
    "Step 2: Create dataset\n",
    "\n",
    "Step 3: drop_last=True drops the last batch if it is shorter than the specified batch_size to prevent loss spikes\n",
    "during training\n",
    "\n",
    "Step 4: The number of CPU processes to use for preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "695fe5f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function is very important and will help create the input output data pairs\n",
    "def create_dataloader_v1(txt, # Text file (dataset that we have)\n",
    "                         batch_size=4, # How many batches to run in parallel\n",
    "                         max_length=256, # Basically equal to the context length (GPT2 or 3 use this size during training)\n",
    "                         stride=128, # When we create input output batches, the stride is how much we need to skip before we create the next batch\n",
    "                         shuffle=True, # Shuffle dataset\n",
    "                         drop_last=True, #\n",
    "                         num_workers=0 # parallel processing on CPU batch that we can run simulataneausly\n",
    "                         ):\n",
    "\n",
    "    # Initialize the tokenizer with GPT\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "    # Creating the dataset\n",
    "    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n",
    "\n",
    "    # Creating the DataLoader\n",
    "    # The DataLoader will be checking the '__getitem__' method in the GPTDatasetV1 class\n",
    "    # it will return the input output pair based on what is mentioned in the '__getitem__'\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        drop_last=drop_last,\n",
    "        num_workers=num_workers\n",
    "    )\n",
    "\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef1fe338",
   "metadata": {},
   "source": [
    "Let's test the dataloader with a batch size of 1 for an LLM with a context size of 4, \n",
    "\n",
    "This will develop an intuition of how the GPTDatasetV1 class and the\n",
    "create_dataloader_v1 function work together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7a077ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe50255",
   "metadata": {},
   "source": [
    "Convert dataloader into a Python iterator to fetch the next entry via Python's built-in next() function\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4b45f009",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pytorch Version: 2.2.2\n",
      "[tensor([[  40,  367, 2885, 1464]]), tensor([[ 367, 2885, 1464, 1807]])]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"Pytorch Version:\", torch.__version__)\n",
    "\n",
    "dataloader = create_dataloader_v1(raw_text, batch_size=1, max_length=4, stride=1, shuffle=False)\n",
    "\n",
    "data_iter = iter(dataloader)\n",
    "first_batch = next(data_iter)\n",
    "print(first_batch) # Input output pair\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2451fbb1",
   "metadata": {},
   "source": [
    "The first_batch variable contains two tensors: the first tensor stores the input token IDs,\n",
    "and the second tensor stores the target token IDs. \n",
    "\n",
    "Since the max_length is set to 4, each of the two tensors contains 4 token IDs. \n",
    "\n",
    "Note that an input size of 4 is relatively small and only chosen for illustration purposes. It is common to train LLMs with input sizes of at least\n",
    "256.\n",
    "\n",
    "More stride means it moves over more spaces!\n",
    "\n",
    "To illustrate the meaning of stride=1, let's fetch another batch from this dataset: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5f385754",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[ 367, 2885, 1464, 1807]]), tensor([[2885, 1464, 1807, 3619]])]\n"
     ]
    }
   ],
   "source": [
    "second_batch = next(data_iter)\n",
    "print(second_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f4ce45",
   "metadata": {},
   "source": [
    "\n",
    "If we compare the first with the second batch, we can see that the second batch's token\n",
    "IDs are shifted by one position compared to the first batch. \n",
    "\n",
    "For example, the second ID in\n",
    "the first batch's input is 367, which is the first ID of the second batch's input. \n",
    "\n",
    "The stride\n",
    "setting dictates the number of positions the inputs shift across batches, emulating a sliding\n",
    "window approach\n",
    "\n",
    "\n",
    "Batch sizes of 1, such as we have sampled from the data loader so far, are useful for\n",
    "illustration purposes. \n",
    "                                                                                 \n",
    "If you have previous experience with deep learning, you may know\n",
    "that small batch sizes require less memory during training but lead to more noisy model\n",
    "updates.\n",
    "\n",
    "Just like in regular deep learning, the batch size is a trade-off and hyperparameter\n",
    "to experiment with when training LLMs.\n",
    "\n",
    "    \n",
    "Before we move on to the two final sections of this chapter that are focused on creating\n",
    "the embedding vectors from the token IDs, let's have a brief look at how we can use the\n",
    "data loader to sample with a batch size greater than 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9c8cd099",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input\n",
      ": tensor([[   40,   367,  2885,  1464],\n",
      "        [ 1807,  3619,   402,   271],\n",
      "        [10899,  2138,   257,  7026],\n",
      "        [15632,   438,  2016,   257],\n",
      "        [  922,  5891,  1576,   438],\n",
      "        [  568,   340,   373,   645],\n",
      "        [ 1049,  5975,   284,   502],\n",
      "        [  284,  3285,   326,    11]])\n",
      "\n",
      "Targets\n",
      ": tensor([[  367,  2885,  1464,  1807],\n",
      "        [ 3619,   402,   271, 10899],\n",
      "        [ 2138,   257,  7026, 15632],\n",
      "        [  438,  2016,   257,   922],\n",
      "        [ 5891,  1576,   438,   568],\n",
      "        [  340,   373,   645,  1049],\n",
      "        [ 5975,   284,   502,   284],\n",
      "        [ 3285,   326,    11,   287]])\n"
     ]
    }
   ],
   "source": [
    "dataloader = create_dataloader_v1(raw_text, batch_size=8, max_length=4, stride=4, shuffle=False)\n",
    "\n",
    "data_iter = iter(dataloader)\n",
    "inputs, targets = next(data_iter)\n",
    "\n",
    "print(f\"Input\\n: {inputs}\")\n",
    "print(f\"\\nTargets\\n: {targets}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "669455b2",
   "metadata": {},
   "source": [
    "Note that we increase the stride to 4. This is to utilize the data set fully (we don't skip a\n",
    "single word) but also avoid any overlap between the batches, since more overlap could lead\n",
    "to increased overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d6abfa8",
   "metadata": {},
   "source": [
    "## Creating Token Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4ee42892",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = torch.tensor([2, 3, 5, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d85dde3",
   "metadata": {},
   "source": [
    "    \n",
    "For the sake of simplicity and illustration purposes, suppose we have a small vocabulary of\n",
    "only 6 words (instead of the 50,257 words in the BPE tokenizer vocabulary), and we want\n",
    "to create embeddings of size 3 (in GPT-3, the embedding size is 12,288 dimensions):\n",
    "\n",
    "Using the vocab_size and output_dim, we can instantiate an embedding layer in PyTorch,\n",
    "setting the random seed to 123 for reproducibility purposes:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "69dbec6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.3374, -0.1778, -0.1690],\n",
      "        [ 0.9178,  1.5810,  1.3010],\n",
      "        [ 1.2753, -0.2010, -0.1606],\n",
      "        [-0.4015,  0.9666, -1.1481],\n",
      "        [-1.1589,  0.3255, -0.6315],\n",
      "        [-2.8400, -0.7849, -1.4096]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 6\n",
    "output_dim = 3\n",
    "\n",
    "# Random\n",
    "torch.manual_seed(123)\n",
    "embedding_layer = torch.nn.Embedding(vocab_size, output_dim)\n",
    "\n",
    "# Prints the embedding matrix\n",
    "print(embedding_layer.weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c559ccc4",
   "metadata": {},
   "source": [
    "    \n",
    "We can see that the weight matrix of the embedding layer contains small, random values.\n",
    "These values are optimized during LLM training as part of the LLM optimization itself, as we\n",
    "will see in upcoming chapters. Moreover, we can see that the weight matrix has six rows\n",
    "and three columns. There is one row for each of the six possible tokens in the vocabulary.\n",
    "And there is one column for each of the three embedding dimensions.\n",
    "    \n",
    "After we instantiated the embedding layer, let's now apply it to a token ID to obtain the\n",
    "embedding vector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4d5769aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.4015,  0.9666, -1.1481]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Getting vector for token IDs, all we do is LOOKUP the Token ID number\n",
    "# Thats why its called a LOOKUP table for the torch.nn.Embedding()\n",
    "print(embedding_layer(torch.tensor([3])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "770326c5",
   "metadata": {},
   "source": [
    "If we compare the embedding vector for token ID 3 to the previous embedding matrix, we\n",
    "see that it is identical to the 4th row (Python starts with a zero index, so it's the row\n",
    "corresponding to index 3). In other words, the embedding layer is essentially a look-up\n",
    "operation that retrieves rows from the embedding layer's weight matrix via a token ID.\n",
    "    \n",
    "\n",
    "Previously, we have seen how to convert a single token ID into a three-dimensional\n",
    "embedding vector. Let's now apply that to all four input IDs we defined earlier\n",
    "(torch.tensor([2, 3, 5, 1])):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bc9783ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.2753, -0.2010, -0.1606],\n",
      "        [-0.4015,  0.9666, -1.1481],\n",
      "        [-2.8400, -0.7849, -1.4096],\n",
      "        [ 0.9178,  1.5810,  1.3010]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Each row in this output matrix is obtained via a lookup operation from the embedding weight matrix\n",
    "print(embedding_layer(input_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7fc6dc1",
   "metadata": {},
   "source": [
    "### POSITIONAL EMBEDDINGS (ENCODING WORD POSITIONS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c3016ee",
   "metadata": {},
   "source": [
    "\n",
    "We now consider more realistic and useful embedding sizes and encode the input\n",
    "tokens into a 256-dimensional vector representation. \n",
    "\n",
    "This is smaller than what the original\n",
    "GPT-3 model used (in GPT-3, the embedding size is 12,288 dimensions) but still reasonable\n",
    "for experimentation. \n",
    "\n",
    "Furthermore, we assume that the token IDs were created by the BPE\n",
    "tokenizer that we implemented earlier, which has a vocabulary size of 50,257:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "92ed6b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 50257\n",
    "output_dim = 256\n",
    "\n",
    "token_embedding_layer = torch.nn.Embedding(vocab_size, output_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "299e1e04",
   "metadata": {},
   "source": [
    "Using the token_embedding_layer above, if we sample data from the data loader, we\n",
    "embed each token in each batch into a 256-dimensional vector. If we have a batch size of 8\n",
    "with four tokens each, the result will be an 8 x 4 x 256 tensor.\n",
    "\n",
    "Let's instantiate the data loader ( Data sampling with a sliding window),\n",
    "first:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "98d1e941",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs:\n",
      " tensor([[   40,   367,  2885,  1464],\n",
      "        [ 1807,  3619,   402,   271],\n",
      "        [10899,  2138,   257,  7026],\n",
      "        [15632,   438,  2016,   257],\n",
      "        [  922,  5891,  1576,   438],\n",
      "        [  568,   340,   373,   645],\n",
      "        [ 1049,  5975,   284,   502],\n",
      "        [  284,  3285,   326,    11]])\n",
      "\n",
      "Inputs shape: torch.Size([8, 4])\n"
     ]
    }
   ],
   "source": [
    "max_length = 4 # Only four input token will be used to predict the next word\n",
    "dataloader = create_dataloader_v1(\n",
    "    raw_text, batch_size=8, max_length=max_length,\n",
    "    stride=max_length, shuffle=False\n",
    ")\n",
    "\n",
    "data_iter = iter(dataloader)\n",
    "inputs, targets = next(data_iter)\n",
    "\n",
    "print(\"Token IDs:\\n\", inputs)\n",
    "print(\"\\nInputs shape:\", inputs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d32865eb",
   "metadata": {},
   "source": [
    "    \n",
    "As we can see, the token ID tensor is 8x4-dimensional, meaning that the data batch\n",
    "consists of 8 text samples with 4 tokens each.\n",
    "    \n",
    "Let's now use the embedding layer to embed these token IDs into 256-dimensional\n",
    "vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7ef833bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 4, 256])\n"
     ]
    }
   ],
   "source": [
    "token_embeddings = token_embedding_layer(inputs)\n",
    "print(token_embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "120f5951",
   "metadata": {},
   "source": [
    "As we can tell based on the 8x4x256-dimensional tensor output, each token ID is now\n",
    "embedded as a 256-dimensional vector.\n",
    "\n",
    "\n",
    "For a GPT model's absolute embedding approach, we just need to create another\n",
    "embedding layer that has the same dimension as the token_embedding_layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7886360",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_length = max_length\n",
    "pos_embedding_layer = torch.nn.Embedding(context_length, output_dim) # For the token embedding the input size was the vocab size while for positional it is context_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ffb8e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 256])\n"
     ]
    }
   ],
   "source": [
    "pos_embeddings = pos_embedding_layer(torch.arange(context_length)) # passing the position is creating the sequence of number until the max length in this case the context length\n",
    "print(pos_embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f63a2595",
   "metadata": {},
   "source": [
    "As shown in the preceding code example, the input to the pos_embeddings is usually a\n",
    "placeholder vector torch.arange(context_length), which contains a sequence of\n",
    "numbers 0, 1, ..., up to the maximum input length  1. \n",
    "\n",
    "The context_length is a variable\n",
    "that represents the supported input size of the LLM. \n",
    "\n",
    "Here, we choose it similar to the\n",
    "maximum length of the input text. \n",
    "\n",
    "In practice, input text can be longer than the supported\n",
    "context length, in which case we have to truncate the text.\n",
    "\n",
    "As we can see, the positional embedding tensor consists of four 256-dimensional vectors.\n",
    "We can now add these directly to the token embeddings, where PyTorch will add the 4x256-\n",
    "dimensional pos_embeddings tensor to each 4x256-dimensional token embedding tensor in\n",
    "each of the 8 batches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "219ebb82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 4, 256])\n"
     ]
    }
   ],
   "source": [
    "input_embeddings = pos_embeddings + token_embeddings\n",
    "print(input_embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c9ab584",
   "metadata": {},
   "source": [
    "The input_embeddings we created are the embedded input\n",
    "examples that can now be processed by the main LLM modules\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
