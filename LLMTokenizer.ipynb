{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "38185e11",
   "metadata": {},
   "source": [
    "# Reading in a short story as text sample into python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8655a9fe",
   "metadata": {},
   "source": [
    "## Step 1: Creating Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad073fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of character: 20479\n",
      "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no \n"
     ]
    }
   ],
   "source": [
    "# Opening Dataset\n",
    "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "\n",
    "print(\"Total number of character:\", len(raw_text))\n",
    "print(raw_text[:99]) # prints the first 100 char of this file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2972c61",
   "metadata": {},
   "source": [
    "### When working with LLMS millions and thousands of books are used, but today we are using only one books just to practice and understand better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db153097",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello,', ' ', 'world.', ' ', 'This,', ' ', 'is', ' ', 'a', ' ', 'test.']\n"
     ]
    }
   ],
   "source": [
    "import re # Splits text to obtain token list, splits any given text based on the whitespaces within the text or any other char\n",
    "\n",
    "text = \"Hello, world. This, is a test.\"\n",
    "result = re.split(r'(\\s)', text) # the '\\s' splits text wherever white spaces are encountered, into individual tokens\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab164fd",
   "metadata": {},
   "source": [
    "### Now lets modify the regular expression splits on whitespaces '(\\s)' and change it to also split where the comma and period to be included as individual splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7a857ff0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', '', ' ', 'world', '.', '', ' ', 'This', ',', '', ' ', 'is', ' ', 'a', ' ', 'test', '.', '']\n"
     ]
    }
   ],
   "source": [
    "result = re.split(r'([,.]|\\s)', text)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3435786",
   "metadata": {},
   "source": [
    "### Now that the commas, periods are also included as its own token we can now remove also the whitespace "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc13b46d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', 'world', '.', 'This', ',', 'is', 'a', 'test', '.']\n"
     ]
    }
   ],
   "source": [
    "# Removing whitespaces completely from the sentence\n",
    "result = [item for item in result if item.strip()] #item.strip() will return true if char in sentence or false for none (Whitespaces will not be returned due to being false)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e2e220",
   "metadata": {},
   "source": [
    "When developing a simple tokenizer, whether we should encode whitespaces as\n",
    "separate characters or just remove them depends on our application and its\n",
    "requirements. Removing whitespaces reduces the memory and computing\n",
    "requirements. However, keeping whitespaces can be useful if we train models that\n",
    "are sensitive to the exact structure of the text (for example, Python code, which is\n",
    "sensitive to indentation and spacing). Here, we remove whitespaces for simplicity\n",
    "and brevity of the tokenized outputs. Later, we will switch to a tokenization scheme\n",
    "that includes whitespaces."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b18d330",
   "metadata": {},
   "source": [
    "### Lets include more char for separate tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d32ec586",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', 'world', '.', 'Is', 'this', '--', 'a', 'test', '?']\n"
     ]
    }
   ],
   "source": [
    "text = \"Hello, world. Is this-- a test?\"\n",
    "\n",
    "# The following two lines of codes are our tokenization scheme\n",
    "result = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text) # Splits texts depending on char\n",
    "result = [item.strip() for item in result if item.strip()] # Remove whitespaces\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09180491",
   "metadata": {},
   "source": [
    "### Now lets apply these two statements of the tokenization scheme into the 'raw_text'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4804603f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'HAD', 'always', 'thought', 'Jack', 'Gisburn', 'rather', 'a', 'cheap', 'genius', '--', 'though', 'a', 'good', 'fellow', 'enough', '--', 'so', 'it', 'was', 'no', 'great', 'surprise', 'to', 'me', 'to', 'hear', 'that', ',', 'in']\n",
      "Token Length: 4690\n"
     ]
    }
   ],
   "source": [
    "preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', raw_text)\n",
    "preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "\n",
    "print(preprocessed[:30]) # prints first 30 char. from file\n",
    "print(\"Token Length:\", len(preprocessed))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46539428",
   "metadata": {},
   "source": [
    "Currently our tokens are not in numerical representation rather still in words/char representations. \n",
    "Next step will be doing **Vocabulary**.\n",
    "\n",
    "The Vocabulary contains unique tokens and token IDs. Should be mapped in alphabetical order. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f991c90d",
   "metadata": {},
   "source": [
    "## Step 2: Creating Token IDs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2826072",
   "metadata": {},
   "source": [
    "In the previous section, we tokenized Edith Wharton's short story and assigned it to a\n",
    "Python variable called preprocessed. Let's now create a list of all unique tokens and sort\n",
    "them alphabetically to determine the vocabulary size:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4e0a0406",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of Vocab: 1130\n"
     ]
    }
   ],
   "source": [
    "# Sorting the set in alphabetical order\n",
    "all_words = sorted(set(preprocessed))\n",
    "\n",
    "# Print Vocab size\n",
    "vocab_len = len(all_words)\n",
    "print(\"Size of Vocab:\", vocab_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a4b1fa",
   "metadata": {},
   "source": [
    "We can see that the vocab size is 1130, indicating that it is less than the size of the *tokens*, which was expected as the vocab size only includes *unique tokens/words*!\n",
    "\n",
    "Remember that a vocab consists of a dictionary of tokens and its associate token IDs\n",
    "\n",
    "After determining that the vocabulary size is 1,130 via the above code, we create the vocabulary and print its first 51 entries for illustration purposes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c4a0767b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('!', 0)\n",
      "('\"', 1)\n",
      "(\"'\", 2)\n",
      "('(', 3)\n",
      "(')', 4)\n",
      "(',', 5)\n",
      "('--', 6)\n",
      "('.', 7)\n",
      "(':', 8)\n",
      "(';', 9)\n",
      "('?', 10)\n",
      "('A', 11)\n",
      "('Ah', 12)\n",
      "('Among', 13)\n",
      "('And', 14)\n",
      "('Are', 15)\n",
      "('Arrt', 16)\n",
      "('As', 17)\n",
      "('At', 18)\n",
      "('Be', 19)\n",
      "('Begin', 20)\n",
      "('Burlington', 21)\n",
      "('But', 22)\n",
      "('By', 23)\n",
      "('Carlo', 24)\n",
      "('Chicago', 25)\n",
      "('Claude', 26)\n",
      "('Come', 27)\n",
      "('Croft', 28)\n",
      "('Destroyed', 29)\n",
      "('Devonshire', 30)\n",
      "('Don', 31)\n",
      "('Dubarry', 32)\n",
      "('Emperors', 33)\n",
      "('Florence', 34)\n",
      "('For', 35)\n",
      "('Gallery', 36)\n",
      "('Gideon', 37)\n",
      "('Gisburn', 38)\n",
      "('Gisburns', 39)\n",
      "('Grafton', 40)\n",
      "('Greek', 41)\n",
      "('Grindle', 42)\n",
      "('Grindles', 43)\n",
      "('HAD', 44)\n",
      "('Had', 45)\n",
      "('Hang', 46)\n",
      "('Has', 47)\n",
      "('He', 48)\n",
      "('Her', 49)\n",
      "('Hermia', 50)\n"
     ]
    }
   ],
   "source": [
    "# we will assign an integer values to all the words in vocab (create a token id)\n",
    "vocab = {token: integer for integer, token in enumerate(all_words)}\n",
    "\n",
    "# printing a visual representation of how the tokens are being assigned\n",
    "for i, item in enumerate(vocab.items()):\n",
    "    print(item)\n",
    "    if i >= 50:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5239321",
   "metadata": {},
   "source": [
    "As we can see, based on the output above, the dictionary contains individual tokens associated with unique integer labels. Later in this book, when we want to convert the outputs of an LLM from numbers back into text, we also need a way to turn token IDs into text. For this, we can create an inverse version of the vocabulary that maps token IDs back to corresponding text tokens.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c1a2646",
   "metadata": {},
   "source": [
    "- Let's implement a complete tokenizer class in Python.\n",
    "\n",
    "    - The class will have an encode method that splits text into tokens and carries out the string-to-integer mapping to produce token IDs via the vocabulary. \n",
    "\n",
    "    - In addition, we implement a decode method that carries out the reverse integer-to-string mapping to convert the token IDs back into text.\n",
    "- Step 1: Store the vocabulary as a class attribute for access in the encode and decode methods\n",
    "\n",
    "- Step 2: Create an inverse vocabulary that maps token IDs back to the original text tokens\n",
    "\n",
    "- Step 3: Process input text into token IDs\n",
    "\n",
    "- Step 4: Convert token IDs back into text\n",
    "\n",
    "- Step 5: Replace spaces before the specified punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1a8e1b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizerV1:\n",
    "    # this init method is called by default when called upon this class 'SimpleTokenizerV1', with the arguments which takes vocab\n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = {i:s for s, i in vocab.items()} # take the token and token_id in vocab and flip it, used for decoder method\n",
    "\n",
    "    # Exact same preprocessing text is introduced as done before for 'Encoder'\n",
    "    def encode(self, text):\n",
    "        preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "\n",
    "        # converting individual token to token ids\n",
    "        ids = [self.str_to_int[s] for s in preprocessed]\n",
    "        return ids\n",
    "\n",
    "    # we are using the reversed dictionary(int_to_str), and converting the token_id to individual tokens, then joining the individual tokens together\n",
    "    def decode(self, ids):\n",
    "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
    "\n",
    "        # Replace spaces before the specified punctuations\n",
    "        text = re.sub(r'\\s+([,.:;\"()\\'])', r'\\1', text) # getting rid of all the spaces before the punctuations to make a complete sentence\n",
    "        return text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "907f410f",
   "metadata": {},
   "source": [
    "### Let's instantiate a new tokenizer object from the SimpleTokenizerV1 class and tokenize a passage from Edith Wharton's short story to try it out in practice:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e94f4995",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 56, 2, 850, 988, 602, 533, 746, 5, 1126, 596, 5, 1, 67, 7, 38, 851, 1108, 754, 793, 7]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = SimpleTokenizerV1(vocab)\n",
    "text = \"\"\"\"It's the last he painted, you know,\"\n",
    "           Mrs. Gisburn said with pardonable pride.\"\"\"\n",
    "\n",
    "ids = tokenizer.encode(text)\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3744304",
   "metadata": {},
   "source": [
    "### Next, let's see if we can turn these token IDs back into text using the decode method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85614ae1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\" It\\' s the last he painted, you know,\" Mrs. Gisburn said with pardonable pride.'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7bcc4e7",
   "metadata": {},
   "source": [
    "The text that was implemented in 'text' was used from the training set the \"raw_text\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a8e835",
   "metadata": {},
   "source": [
    "### Let's now apply it to a new text sample that is not contained in the training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2984f502",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Hello'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHello do you like Tea?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28mprint\u001b[39m(tokenizer\u001b[38;5;241m.\u001b[39mencode(text))\n",
      "Cell \u001b[0;32mIn[20], line 13\u001b[0m, in \u001b[0;36mSimpleTokenizerV1.encode\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m     10\u001b[0m preprocessed \u001b[38;5;241m=\u001b[39m [item\u001b[38;5;241m.\u001b[39mstrip() \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m preprocessed \u001b[38;5;28;01mif\u001b[39;00m item\u001b[38;5;241m.\u001b[39mstrip()]\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# converting individual token to token ids\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m ids \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstr_to_int[s] \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m preprocessed]\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ids\n",
      "Cell \u001b[0;32mIn[20], line 13\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     10\u001b[0m preprocessed \u001b[38;5;241m=\u001b[39m [item\u001b[38;5;241m.\u001b[39mstrip() \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m preprocessed \u001b[38;5;28;01mif\u001b[39;00m item\u001b[38;5;241m.\u001b[39mstrip()]\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# converting individual token to token ids\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m ids \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstr_to_int[s] \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m preprocessed]\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ids\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Hello'"
     ]
    }
   ],
   "source": [
    "text = \"Hello do you like Tea?\"\n",
    "print(tokenizer.encode(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0adb7ddd",
   "metadata": {},
   "source": [
    "### An error is printed due to hello not being used in the 'vocab', which is the short story (raw_text). This highlights the need to consider large and diverse training sets to extend the vocabulary when working on LLMs. To deal with this issue we add special context tokens. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2bcc5f0",
   "metadata": {},
   "source": [
    "## ADDING SPECIAL CONTEXT TOKENS\n",
    "\n",
    "- In this section, we will modify this tokenizer to handle unknown\n",
    "words.\n",
    "\n",
    "- In particular, we will modify the vocabulary and tokenizer we implemented in the\n",
    "previous section, SimpleTokenizerV2, to support two new tokens, <|unk|> and\n",
    "<|endoftext|>\n",
    "\n",
    "- We can modify the tokenizer to use an <|unk|> token if it\n",
    "encounters a word that is not part of the vocabulary. \n",
    "\n",
    "- Furthermore, we add a token between\n",
    "unrelated texts. \n",
    "\n",
    "- For example, when training GPT-like LLMs on multiple independent\n",
    "documents or books, it is common to insert a token before each document or book that\n",
    "follows a previous text source\n",
    "\n",
    "- When working with multiple text sources, we add <|endoftext|> tokens between these texts. These tokens act as markers, signaling the start and end of a particular segment. This leads to more effective processing and understanding by the LLMs.\n",
    "  - if this token was not there it would join all text and mix everything up."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "684f60be",
   "metadata": {},
   "source": [
    "### Now lets modify the vocabulary to include these two special tokens, <|unk|> and <|endoftext|>, by adding these to the list of all unique words that we created in the previous section:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4d02e52b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('younger', 1127)\n",
      "('your', 1128)\n",
      "('yourself', 1129)\n",
      "('<|endoftext|>', 1130)\n",
      "('<|unk|>', 1131)\n"
     ]
    }
   ],
   "source": [
    "all_tokens = sorted(list(set(preprocessed)))\n",
    "\n",
    "# to add the new tokens use the .extend\n",
    "all_tokens.extend([\"<|endoftext|>\", \"<|unk|>\"])\n",
    "\n",
    "vocab = {token:integer for integer, token in enumerate(all_tokens)}\n",
    "\n",
    "len(vocab.items())\n",
    "for i, item in enumerate(list(vocab.items()) [-5:]):\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2504d77e",
   "metadata": {},
   "source": [
    "### Now lets update the SimpleTokenizer in our second version by replacing unknown words and spaces before the specified punctuations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f0619e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizerV2:\n",
    "    # Remains the same\n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = {i:s for s,i in vocab.items()}\n",
    "\n",
    "    def encode(self, text):\n",
    "        preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "\n",
    "        # If not presented in vocab the token will be ID as unknown\n",
    "        preprocessed = [item if item in self.str_to_int else \"<|unk|>\" for item in preprocessed]\n",
    "\n",
    "        # converting individual token to token ids\n",
    "        ids = [self.str_to_int[s] for s in preprocessed]\n",
    "        return ids\n",
    "\n",
    "    # Stays the same\n",
    "    def decode(self, ids):\n",
    "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
    "        text = re.sub(r'\\s+([,.:;\"()\\'])', r'\\1', text) # getting rid of all the spaces before the punctuations to make a complete sentence\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9f58fd89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, do you like Tea?<|endoftext|>In the sunlit terraces of the palace.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = SimpleTokenizerV2(vocab)\n",
    "\n",
    "# Splitting text source like done in GPT\n",
    "text1 = \"Hello, do you like Tea?\"\n",
    "text2 = \"In the sunlit terraces of the palace.\"\n",
    "\n",
    "text = \"<|endoftext|>\".join((text1,text2))\n",
    "\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c5c6eb7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1131, 5, 355, 1126, 628, 1131, 10, 1131, 988, 956, 984, 722, 988, 1131, 7]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'<|unk|>, do you like <|unk|> ? <|unk|> the sunlit terraces of the <|unk|>.'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# No more errors due to dealing with unkown char\n",
    "print(tokenizer.encode(text))\n",
    "tokenizer.decode(tokenizer.encode(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0073e17",
   "metadata": {},
   "source": [
    "\n",
    "So far, we have discussed tokenization as an essential step in processing text as input to\n",
    "LLMs. Depending on the LLM, some researchers also consider additional special tokens such\n",
    "as the following:\n",
    "\n",
    "[BOS] (beginning of sequence): This token marks the start of a text. It\n",
    "signifies to the LLM where a piece of content begins.\n",
    "\n",
    "[EOS] (end of sequence): This token is positioned at the end of a text,\n",
    "and is especially useful when concatenating multiple unrelated texts,\n",
    "similar to <|endoftext|>. For instance, when combining two different\n",
    "Wikipedia articles or books, the [EOS] token indicates where one article\n",
    "ends and the next one begins.\n",
    "\n",
    "[PAD] (padding): When training LLMs with batch sizes larger than one,\n",
    "the batch might contain texts of varying lengths. To ensure all texts have\n",
    "the same length, the shorter texts are extended or \"padded\" using the\n",
    "[PAD] token, up to the length of the longest text in the batch.\n",
    "\n",
    "Note that the tokenizer used for GPT models does not need any of these tokens mentioned\n",
    "above but only uses an <|endoftext|> token for simplicity\n",
    "\n",
    "the tokenizer used for GPT models also doesn't use an <|unk|> token for outof-vocabulary words. Instead, GPT models use a byte pair encoding tokenizer, which breaks\n",
    "down words into subword units"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
